{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, activations\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tempfile\n",
    "import zipfile\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECTROGRAM = False             # If True, model is trained on spectrograms. If False, model is trained on mel-spectrograms.\n",
    "classes = [\"Anger\", \"Disgust\", \"Fear\", \"Happy\", \"Neutral\", \"Sad\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SPECTROGRAM == True:\n",
    "    path_train = \"/Speech_emotion_recognition/New_Big_dataset_Spetember2022/Train_6_emotions/Spectrogram\"\n",
    "    path_test = \"/Speech_emotion_recognition/New_Big_dataset_Spetember2022/Test_6_emotions/Spectrogram\"\n",
    "    log_directory = \"/Speech_emotion_recognition/Testy_do_mgr/logs/TensorBoard/6_emotions_spec\"\n",
    "    filepath = '/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_spec.h5'\n",
    "    log_directory_on_4_emotions = \"/Speech_emotion_recognition/Testy_do_mgr/logs/TensorBoard/4_emotions_on_model_trained_on_6_emotions_spec\"\n",
    "    filepath_on_4_emotions = '/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_spec.h5'\n",
    "\n",
    "else:\n",
    "    path_train = \"/Speech_emotion_recognition/New_Big_dataset_Spetember2022/Train_6_emotions/Melspectrogram\"\n",
    "    path_test = \"/Speech_emotion_recognition/New_Big_dataset_Spetember2022/Test_6_emotions/Melspectrogram\"\n",
    "    log_directory = \"/Speech_emotion_recognition/Testy_do_mgr/logs/TensorBoard/6_emotions_mel\"\n",
    "    filepath = '/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5'\n",
    "    log_directory_on_4_emotions = \"/Speech_emotion_recognition/Testy_do_mgr/logs/TensorBoard/4_emotions_on_model_trained_on_6_emotions_mel\"\n",
    "    filepath_on_4_emotions = '/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train_data_path):\n",
    "    \"\"\"\n",
    "    Loads train data from all datasets, for IEMOCAP data from all sessions except session 2\n",
    "\n",
    "    Returns:\n",
    "        data_train - training samples\n",
    "        data_val - validation samples\n",
    "        target_train - training targets\n",
    "        target_val - validation targets\n",
    "        \n",
    "    \"\"\"\n",
    "    classNumber = 0\n",
    "    targets = []\n",
    "    img = []\n",
    "\n",
    "    for current_folder in classes:\n",
    "        emotion_folder = train_data_path + '/' + current_folder\n",
    "        for fileName in os.listdir(emotion_folder):\n",
    "            targets.append(classNumber)\n",
    "            img.append(np.array(Image.open(emotion_folder + '/' + fileName).convert('RGB'))/255)\n",
    "        classNumber += 1\n",
    "\n",
    "    \n",
    "    targets_array = np.asarray(targets)\n",
    "    targets = []\n",
    "    img_array = np.asarray(img)\n",
    "    img = []\n",
    "\n",
    "    targets_array = tf.keras.utils.to_categorical(targets_array)\n",
    "    data_train, data_val, target_train, target_val = train_test_split(img_array, targets_array, test_size=0.25, random_state=0)\n",
    "\n",
    "    print(\"shapes\")\n",
    "    print(data_train.shape)\n",
    "    print(target_train.shape)\n",
    "    print(data_val.shape)\n",
    "    print(target_val.shape)\n",
    "\n",
    "    return data_train, data_val, target_train, target_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_4_emotions(train_data_path):\n",
    "    \"\"\"\n",
    "    Loads train data from all datasets, for IEMOCAP data from all sessions except session 2\n",
    "\n",
    "    Returns:\n",
    "        data_train - training samples\n",
    "        data_val - validation samples\n",
    "        target_train - training targets\n",
    "        target_val - validation targets\n",
    "        \n",
    "    \"\"\"\n",
    "    classNumber = 0\n",
    "    targets = []\n",
    "    img = []\n",
    "\n",
    "    for current_folder in classes:\n",
    "        if(current_folder != \"Disgust\" and current_folder != \"Fear\"):\n",
    "            emotion_folder = train_data_path + '/' + current_folder\n",
    "            for fileName in os.listdir(emotion_folder):\n",
    "                targets.append(classNumber)\n",
    "                img.append(np.array(Image.open(emotion_folder + '/' + fileName).convert('RGB'))/255)\n",
    "            classNumber += 1\n",
    "\n",
    "    \n",
    "    targets_array = np.asarray(targets)\n",
    "    targets = []\n",
    "    img_array = np.asarray(img)\n",
    "    img = []\n",
    "\n",
    "    targets_array = tf.keras.utils.to_categorical(targets_array)\n",
    "    data_train, data_val, target_train, target_val = train_test_split(img_array, targets_array, test_size=0.25, random_state=0)\n",
    "\n",
    "    print(\"shapes\")\n",
    "    print(data_train.shape)\n",
    "    print(target_train.shape)\n",
    "    print(data_val.shape)\n",
    "    print(target_val.shape)\n",
    "\n",
    "    return data_train, data_val, target_train, target_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data_4_emotions(test_data_path):\n",
    "    \"\"\"\n",
    "    Loads test data from all datasets, for IEMOCAP data from all sessions except session 2\n",
    "\n",
    "    Returns:\n",
    "        data_test - test samples\n",
    "        target_test_to_categorical - test targets\n",
    "        \n",
    "    \"\"\"\n",
    "    classNumber = 0\n",
    "    targets = []\n",
    "    img = []\n",
    "\n",
    "    for current_folder in classes:\n",
    "        if(current_folder != \"Disgust\" and current_folder != \"Fear\"):\n",
    "            emotion_folder = test_data_path + '/' + current_folder\n",
    "            for fileName in os.listdir(emotion_folder):\n",
    "                targets.append(classNumber)\n",
    "                img.append(np.array(Image.open(emotion_folder + '/' + fileName).convert('RGB'))/255)\n",
    "            classNumber += 1\n",
    "\n",
    "    data_test = np.asarray(img)\n",
    "    img = []\n",
    "    target_test = np.asarray(targets)\n",
    "    targets = []\n",
    "    target_test_to_categorical = tf.keras.utils.to_categorical(target_test)\n",
    "\n",
    "    print(data_test.shape)\n",
    "    print(target_test_to_categorical.shape)\n",
    "\n",
    "    return data_test, target_test_to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test_data_path):\n",
    "    \"\"\"\n",
    "    Loads test data from all datasets, for IEMOCAP data from all sessions except session 2\n",
    "\n",
    "    Returns:\n",
    "        data_test - test samples\n",
    "        target_test_to_categorical - test targets\n",
    "        \n",
    "    \"\"\"\n",
    "    classNumber = 0\n",
    "    targets = []\n",
    "    img = []\n",
    "\n",
    "    for current_folder in classes:\n",
    "        emotion_folder = test_data_path + '/' + current_folder\n",
    "        for fileName in os.listdir(emotion_folder):\n",
    "            targets.append(classNumber)\n",
    "            img.append(np.array(Image.open(emotion_folder + '/' + fileName).convert('RGB'))/255)\n",
    "        classNumber += 1\n",
    "\n",
    "    data_test = np.asarray(img)\n",
    "    img = []\n",
    "    target_test = np.asarray(targets)\n",
    "    targets = []\n",
    "    target_test_to_categorical = tf.keras.utils.to_categorical(target_test)\n",
    "\n",
    "    print(data_test.shape)\n",
    "    print(target_test_to_categorical.shape)\n",
    "\n",
    "    return data_test, target_test_to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SHEDULER\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 15:\n",
    "        eta = lr\n",
    "    else:\n",
    "        eta = lr - 0.00001\n",
    "    if lr < 0.00005:\n",
    "        eta = 0.00005\n",
    "\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get train data and define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes\n",
      "(6047, 231, 349, 3)\n",
      "(6047, 6)\n",
      "(2016, 231, 349, 3)\n",
      "(2016, 6)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, target_train, target_val = get_train_data(path_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 231, 349, 75)      5700      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 77, 116, 75)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 77, 116, 135)      253260    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 38, 135)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25, 38, 135)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 38, 75)        253200    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 12, 75)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 12, 75)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 45)                324045    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 45)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 6)                 276       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 836,481\n",
      "Trainable params: 836,481\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 16:47:50.361622: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-08 16:47:51.077200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22839 MB memory:  -> device: 0, name: TITAN RTX, pci bus id: 0000:09:00.0, compute capability: 7.5\n",
      "2022-09-08 16:47:51.078650: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22576 MB memory:  -> device: 1, name: TITAN RTX, pci bus id: 0000:41:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# MODEL SHEDULER\n",
    "scheduler_callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "# TENSORBOARD\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_directory)\n",
    "\n",
    "# SAVE MODEL \n",
    "checkpoint = ModelCheckpoint(filepath,monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(75, (5, 5), activation='relu', input_shape=(231, 349, 3), padding='same'))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "model.add(layers.Conv2D(135, (5, 5), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "model.add(layers.Dropout(0.15))\n",
    "model.add(layers.Conv2D(75, (5, 5), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D((3, 3)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Flatten()) \n",
    "model.add(layers.Dense(45, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(6, activation='softmax'))\n",
    "\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model compile and model fit to start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 349, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPILE \n",
    "model.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-08 16:47:58.819362: I tensorflow/stream_executor/cuda/cuda_dnn.cc:366] Loaded cuDNN version 8101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189/189 [==============================] - ETA: 0s - loss: 1.6787 - accuracy: 0.2560\n",
      "Epoch 00001: val_loss improved from inf to 1.51600, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 17s 73ms/step - loss: 1.6787 - accuracy: 0.2560 - val_loss: 1.5160 - val_accuracy: 0.3239\n",
      "Epoch 2/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.5285 - accuracy: 0.3403\n",
      "Epoch 00002: val_loss improved from 1.51600 to 1.39308, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.5285 - accuracy: 0.3403 - val_loss: 1.3931 - val_accuracy: 0.4320\n",
      "Epoch 3/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.4634 - accuracy: 0.3718\n",
      "Epoch 00003: val_loss improved from 1.39308 to 1.34305, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.4634 - accuracy: 0.3718 - val_loss: 1.3430 - val_accuracy: 0.4638\n",
      "Epoch 4/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.4209 - accuracy: 0.3850\n",
      "Epoch 00004: val_loss improved from 1.34305 to 1.30312, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.4206 - accuracy: 0.3850 - val_loss: 1.3031 - val_accuracy: 0.4752\n",
      "Epoch 5/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.4013 - accuracy: 0.4043\n",
      "Epoch 00005: val_loss improved from 1.30312 to 1.28046, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.4021 - accuracy: 0.4037 - val_loss: 1.2805 - val_accuracy: 0.4926\n",
      "Epoch 6/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3670 - accuracy: 0.4230\n",
      "Epoch 00006: val_loss did not improve from 1.28046\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.3670 - accuracy: 0.4230 - val_loss: 1.2949 - val_accuracy: 0.4990\n",
      "Epoch 7/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.3425 - accuracy: 0.4255\n",
      "Epoch 00007: val_loss improved from 1.28046 to 1.23328, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.3431 - accuracy: 0.4253 - val_loss: 1.2333 - val_accuracy: 0.5025\n",
      "Epoch 8/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3266 - accuracy: 0.4348\n",
      "Epoch 00008: val_loss did not improve from 1.23328\n",
      "189/189 [==============================] - 11s 56ms/step - loss: 1.3266 - accuracy: 0.4348 - val_loss: 1.2422 - val_accuracy: 0.5134\n",
      "Epoch 9/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.3177 - accuracy: 0.4427\n",
      "Epoch 00009: val_loss improved from 1.23328 to 1.23087, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.3177 - accuracy: 0.4427 - val_loss: 1.2309 - val_accuracy: 0.5188\n",
      "Epoch 10/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.2930 - accuracy: 0.4510\n",
      "Epoch 00010: val_loss improved from 1.23087 to 1.21786, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.2940 - accuracy: 0.4511 - val_loss: 1.2179 - val_accuracy: 0.5233\n",
      "Epoch 11/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.2816 - accuracy: 0.4559\n",
      "Epoch 00011: val_loss improved from 1.21786 to 1.19465, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.2816 - accuracy: 0.4559 - val_loss: 1.1946 - val_accuracy: 0.5332\n",
      "Epoch 12/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.2707 - accuracy: 0.4710\n",
      "Epoch 00012: val_loss did not improve from 1.19465\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.2707 - accuracy: 0.4710 - val_loss: 1.2009 - val_accuracy: 0.5169\n",
      "Epoch 13/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.2554 - accuracy: 0.4712\n",
      "Epoch 00013: val_loss improved from 1.19465 to 1.18845, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.2549 - accuracy: 0.4715 - val_loss: 1.1885 - val_accuracy: 0.5402\n",
      "Epoch 14/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.2436 - accuracy: 0.4849\n",
      "Epoch 00014: val_loss did not improve from 1.18845\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.2438 - accuracy: 0.4844 - val_loss: 1.1921 - val_accuracy: 0.5561\n",
      "Epoch 15/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.2271 - accuracy: 0.4932\n",
      "Epoch 00015: val_loss did not improve from 1.18845\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.2265 - accuracy: 0.4935 - val_loss: 1.1929 - val_accuracy: 0.5575\n",
      "Epoch 16/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.2179 - accuracy: 0.4973\n",
      "Epoch 00016: val_loss did not improve from 1.18845\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.2179 - accuracy: 0.4973 - val_loss: 1.2153 - val_accuracy: 0.5342\n",
      "Epoch 17/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.2075 - accuracy: 0.5027\n",
      "Epoch 00017: val_loss did not improve from 1.18845\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.2081 - accuracy: 0.5022 - val_loss: 1.1924 - val_accuracy: 0.5645\n",
      "Epoch 18/20\n",
      "189/189 [==============================] - ETA: 0s - loss: 1.2192 - accuracy: 0.5009\n",
      "Epoch 00018: val_loss improved from 1.18845 to 1.17220, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.2192 - accuracy: 0.5009 - val_loss: 1.1722 - val_accuracy: 0.5704\n",
      "Epoch 19/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.1888 - accuracy: 0.5108\n",
      "Epoch 00019: val_loss improved from 1.17220 to 1.16175, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 58ms/step - loss: 1.1893 - accuracy: 0.5108 - val_loss: 1.1618 - val_accuracy: 0.5928\n",
      "Epoch 20/20\n",
      "188/189 [============================>.] - ETA: 0s - loss: 1.1745 - accuracy: 0.5221\n",
      "Epoch 00020: val_loss improved from 1.16175 to 1.15521, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/6_emotions_mel.h5\n",
      "189/189 [==============================] - 11s 57ms/step - loss: 1.1747 - accuracy: 0.5217 - val_loss: 1.1552 - val_accuracy: 0.5809\n"
     ]
    }
   ],
   "source": [
    "# MODEL FIT \n",
    "history = model.fit(data_train, target_train,\n",
    "            batch_size = 32,\n",
    "            epochs=20,\n",
    "            shuffle=True,\n",
    "            validation_data=(data_val, target_val),\n",
    "            callbacks=[tensorboard_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get test data, load weights of the best model and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2725, 231, 349, 3)\n",
      "(2725, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_test, target_test_to_categorical = get_test_data(path_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2725/2725 [==============================] - 10s 4ms/step - loss: 1.3673 - accuracy: 0.4859\n",
      "test loss, test acc: [1.36733877658844, 0.4858715534210205]\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(filepath=filepath)\n",
    "    \n",
    "results = model.evaluate(data_test, target_test_to_categorical, batch_size=1)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove last layer of model and add new one suited for 4-emotions classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 231, 349, 75)      5700      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 77, 116, 75)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 77, 116, 135)      253260    \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 38, 135)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 25, 38, 135)       0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 38, 75)        253200    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 8, 12, 75)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 12, 75)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 7200)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 45)                324045    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 45)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 4)                 184       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 836,389\n",
      "Trainable params: 836,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_4_emotions = models.Sequential()\n",
    "for layer in model.layers[:-1]: # go through until last layer\n",
    "    model_4_emotions.add(layer)\n",
    "model_4_emotions.add(layers.Dense(4, activation='softmax'))\n",
    "model_4_emotions.summary()\n",
    "#model_4_emotions.compile(optimizer='adam', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop 2 emotions for optimalization purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapes\n",
      "(3995, 231, 349, 3)\n",
      "(3995, 4)\n",
      "(1332, 231, 349, 3)\n",
      "(1332, 4)\n"
     ]
    }
   ],
   "source": [
    "data_train, data_val, target_train, target_val = get_train_data_4_emotions(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPILE \n",
    "model_4_emotions.compile(optimizer='adam',\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 1.4984 - accuracy: 0.2688\n",
      "Epoch 00001: val_loss improved from inf to 1.32393, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5\n",
      "125/125 [==============================] - 9s 71ms/step - loss: 1.4984 - accuracy: 0.2688 - val_loss: 1.3239 - val_accuracy: 0.3896\n",
      "Epoch 2/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 1.0856 - accuracy: 0.4642\n",
      "Epoch 00002: val_loss improved from 1.32393 to 0.83835, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 1.0842 - accuracy: 0.4648 - val_loss: 0.8384 - val_accuracy: 0.7065\n",
      "Epoch 3/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.8091 - accuracy: 0.6452\n",
      "Epoch 00003: val_loss improved from 0.83835 to 0.72996, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.8079 - accuracy: 0.6463 - val_loss: 0.7300 - val_accuracy: 0.7237\n",
      "Epoch 4/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.7097 - accuracy: 0.6888\n",
      "Epoch 00004: val_loss improved from 0.72996 to 0.63784, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.7097 - accuracy: 0.6889 - val_loss: 0.6378 - val_accuracy: 0.7470\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.6498 - accuracy: 0.7287\n",
      "Epoch 00005: val_loss improved from 0.63784 to 0.61329, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.6498 - accuracy: 0.7287 - val_loss: 0.6133 - val_accuracy: 0.7530\n",
      "Epoch 6/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.6087 - accuracy: 0.7480\n",
      "Epoch 00006: val_loss did not improve from 0.61329\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.6079 - accuracy: 0.7487 - val_loss: 0.6551 - val_accuracy: 0.7297\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - ETA: 0s - loss: 0.5689 - accuracy: 0.7677\n",
      "Epoch 00007: val_loss improved from 0.61329 to 0.60782, saving model to /home/studenci/165122/Speech_emotion_recognition/Testy_do_mgr/logs/SavedModels/4_emotions_on_model_trained_on_6_emotions_mel.h5\n",
      "125/125 [==============================] - 7s 58ms/step - loss: 0.5689 - accuracy: 0.7677 - val_loss: 0.6078 - val_accuracy: 0.7598\n",
      "Epoch 8/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5568 - accuracy: 0.7626\n",
      "Epoch 00008: val_loss did not improve from 0.60782\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.5582 - accuracy: 0.7630 - val_loss: 0.6327 - val_accuracy: 0.7538\n",
      "Epoch 9/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5345 - accuracy: 0.7850\n",
      "Epoch 00009: val_loss did not improve from 0.60782\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.5332 - accuracy: 0.7855 - val_loss: 0.6182 - val_accuracy: 0.7718\n",
      "Epoch 10/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.4861 - accuracy: 0.8057\n",
      "Epoch 00010: val_loss did not improve from 0.60782\n",
      "125/125 [==============================] - 7s 57ms/step - loss: 0.4858 - accuracy: 0.8060 - val_loss: 0.6236 - val_accuracy: 0.7575\n"
     ]
    }
   ],
   "source": [
    "# TENSORBOARD\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_directory_on_4_emotions)\n",
    "\n",
    "# SAVE MODEL \n",
    "checkpoint = ModelCheckpoint(filepath_on_4_emotions,monitor='val_loss',verbose=1,save_best_only=True,mode='min')\n",
    "\n",
    "# MODEL FIT \n",
    "history = model_4_emotions.fit(data_train, target_train,\n",
    "            batch_size = 32,\n",
    "            epochs=10,\n",
    "            shuffle=True,\n",
    "            validation_data=(data_val, target_val),\n",
    "            callbacks=[tensorboard_callback, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1808, 231, 349, 3)\n",
      "(1808, 4)\n"
     ]
    }
   ],
   "source": [
    "data_test, target_test_to_categorical = get_test_data_4_emotions(path_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1808/1808 [==============================] - 7s 4ms/step - loss: 0.6457 - accuracy: 0.7517\n",
      "test loss, test acc: [0.6457031965255737, 0.7516592741012573]\n"
     ]
    }
   ],
   "source": [
    "model_4_emotions.load_weights(filepath=filepath_on_4_emotions)\n",
    "    \n",
    "results = model_4_emotions.evaluate(data_test, target_test_to_categorical, batch_size=1)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.ConstantSparsity(0.5, begin_step=0, frequency=100)\n",
    "  }\n",
    "\n",
    "callbacks = [\n",
    "  tfmot.sparsity.keras.UpdatePruningStep()\n",
    "]\n",
    "\n",
    "pruned_model = prune_low_magnitude(model_4_emotions, **pruning_params)\n",
    "\n",
    "# Odpowiednio mała wartość `lr` dla etapu dotrenowania\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "pruned_model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  5/125 [>.............................] - ETA: 6s - loss: 0.5430 - accuracy: 0.7875WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0208s vs `on_train_batch_end` time: 0.0324s). Check your callbacks.\n",
      "125/125 [==============================] - 11s 67ms/step - loss: 0.5773 - accuracy: 0.7504 - val_loss: 0.7333 - val_accuracy: 0.6464\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 7s 60ms/step - loss: 0.7054 - accuracy: 0.6761 - val_loss: 0.7054 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 7s 60ms/step - loss: 0.6926 - accuracy: 0.6733 - val_loss: 0.6931 - val_accuracy: 0.6839\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 7s 60ms/step - loss: 0.6835 - accuracy: 0.6896 - val_loss: 0.6856 - val_accuracy: 0.6884\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 8s 66ms/step - loss: 0.6570 - accuracy: 0.6936 - val_loss: 0.6791 - val_accuracy: 0.6974\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 7s 60ms/step - loss: 0.6559 - accuracy: 0.7036 - val_loss: 0.6732 - val_accuracy: 0.7012\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 8s 60ms/step - loss: 0.6385 - accuracy: 0.7169 - val_loss: 0.6679 - val_accuracy: 0.7177\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 8s 61ms/step - loss: 0.6318 - accuracy: 0.7166 - val_loss: 0.6637 - val_accuracy: 0.7267\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 8s 65ms/step - loss: 0.6408 - accuracy: 0.7234 - val_loss: 0.6611 - val_accuracy: 0.7297\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 7s 59ms/step - loss: 0.6263 - accuracy: 0.7217 - val_loss: 0.6583 - val_accuracy: 0.7297\n"
     ]
    }
   ],
   "source": [
    "# Dotrenowanie modelu\n",
    "pruned_model.fit(\n",
    "  data_train,\n",
    "  target_train,\n",
    "  epochs=10,\n",
    "  validation_data=(data_val, target_val),\n",
    "  callbacks=callbacks)\n",
    "\n",
    "stripped_pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " prune_low_magnitude_conv2d   (None, 231, 349, 75)     11327     \n",
      " (PruneLowMagnitude)                                             \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 77, 116, 75)      1         \n",
      " ling2d (PruneLowMagnitude)                                      \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 77, 116, 135)     506387    \n",
      " 1 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 25, 38, 135)      1         \n",
      " ling2d_1 (PruneLowMagnitude                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_dropout  (None, 25, 38, 135)      1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_conv2d_  (None, 25, 38, 75)       506327    \n",
      " 2 (PruneLowMagnitude)                                           \n",
      "                                                                 \n",
      " prune_low_magnitude_max_poo  (None, 8, 12, 75)        1         \n",
      " ling2d_2 (PruneLowMagnitude                                     \n",
      " )                                                               \n",
      "                                                                 \n",
      " prune_low_magnitude_dropout  (None, 8, 12, 75)        1         \n",
      " _1 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_flatten  (None, 7200)             1         \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      " prune_low_magnitude_dense (  (None, 45)               648047    \n",
      " PruneLowMagnitude)                                              \n",
      "                                                                 \n",
      " prune_low_magnitude_dropout  (None, 45)               1         \n",
      " _2 (PruneLowMagnitude)                                          \n",
      "                                                                 \n",
      " prune_low_magnitude_dense_2  (None, 4)                366       \n",
      "  (PruneLowMagnitude)                                            \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,672,461\n",
      "Trainable params: 836,389\n",
      "Non-trainable params: 836,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "stripped_pruned_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_weights_sparsity(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            if \"kernel\" not in weight.name or \"centroid\" in weight.name:\n",
    "                continue\n",
    "            weight_size = weight.numpy().size\n",
    "            zero_num = np.count_nonzero(weight == 0)\n",
    "            print(\n",
    "                f\"{weight.name}: {zero_num/weight_size:.2%} sparsity \",\n",
    "                f\"({zero_num}/{weight_size})\",\n",
    "            )\n",
    "\n",
    "def print_model_weight_clusters(model):\n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, tf.keras.layers.Wrapper):\n",
    "            weights = layer.trainable_weights\n",
    "        else:\n",
    "            weights = layer.weights\n",
    "        for weight in weights:\n",
    "            # ignore auxiliary quantization weights\n",
    "            if \"quantize_layer\" in weight.name:\n",
    "                continue\n",
    "            if \"kernel\" in weight.name:\n",
    "                unique_count = len(np.unique(weight))\n",
    "                print(\n",
    "                    f\"{layer.name}/{weight.name}: {unique_count} clusters \"\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sparsity:\n",
      "\n",
      "conv2d/kernel:0: 50.01% sparsity  (2813/5625)\n",
      "conv2d_1/kernel:0: 50.00% sparsity  (126563/253125)\n",
      "conv2d_2/kernel:0: 50.00% sparsity  (126563/253125)\n",
      "dense/kernel:0: 50.00% sparsity  (162000/324000)\n",
      "dense_2/kernel:0: 50.00% sparsity  (90/180)\n",
      "\n",
      "Model clusters:\n",
      "\n",
      "conv2d/conv2d/kernel:0: 2813 clusters \n",
      "conv2d_1/conv2d_1/kernel:0: 126194 clusters \n",
      "conv2d_2/conv2d_2/kernel:0: 126321 clusters \n",
      "dense/dense/kernel:0: 161536 clusters \n",
      "dense_2/dense_2/kernel:0: 91 clusters \n"
     ]
    }
   ],
   "source": [
    "print(\"Model sparsity:\\n\")\n",
    "print_model_weights_sparsity(stripped_pruned_model)\n",
    "\n",
    "print(\"\\nModel clusters:\\n\")\n",
    "print_model_weight_clusters(stripped_pruned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.clustering.keras.experimental import (\n",
    "    cluster,\n",
    ")\n",
    "\n",
    "cluster_weights = tfmot.clustering.keras.cluster_weights\n",
    "CentroidInitialization = tfmot.clustering.keras.CentroidInitialization\n",
    "\n",
    "cluster_weights = cluster.cluster_weights\n",
    "\n",
    "clustering_params = {\n",
    "  'number_of_clusters': 8,\n",
    "  'cluster_centroids_init': CentroidInitialization.KMEANS_PLUS_PLUS,\n",
    "  'preserve_sparsity': True\n",
    "}\n",
    "\n",
    "sparsity_clustered_model = cluster_weights(stripped_pruned_model, **clustering_params)\n",
    "\n",
    "sparsity_clustered_model.compile(\n",
    "            loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uczenie modelu z współdzieleniem wag i zachowaniem przerzedzenia:\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 10s 69ms/step - loss: 0.6256 - accuracy: 0.8038 - val_loss: 0.7113 - val_accuracy: 0.7207\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 0.5584 - accuracy: 0.8263 - val_loss: 0.6845 - val_accuracy: 0.7380\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.5220 - accuracy: 0.8338 - val_loss: 0.6677 - val_accuracy: 0.7432\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.5005 - accuracy: 0.8353 - val_loss: 0.6601 - val_accuracy: 0.7410\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 8s 61ms/step - loss: 0.4846 - accuracy: 0.8350 - val_loss: 0.6505 - val_accuracy: 0.7477\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 8s 63ms/step - loss: 0.4668 - accuracy: 0.8390 - val_loss: 0.6431 - val_accuracy: 0.7508\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 8s 64ms/step - loss: 0.4507 - accuracy: 0.8463 - val_loss: 0.6377 - val_accuracy: 0.7500\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 8s 61ms/step - loss: 0.4377 - accuracy: 0.8498 - val_loss: 0.6319 - val_accuracy: 0.7530\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.4248 - accuracy: 0.8563 - val_loss: 0.6232 - val_accuracy: 0.7620\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 8s 62ms/step - loss: 0.4142 - accuracy: 0.8563 - val_loss: 0.6205 - val_accuracy: 0.7575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3fcbf59400>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Uczenie modelu z współdzieleniem wag i zachowaniem przerzedzenia:')\n",
    "sparsity_clustered_model.fit(\n",
    "  data_train,\n",
    "  target_train,\n",
    "  epochs=10,\n",
    "  validation_data=(data_val, target_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " cluster_conv2d (ClusterWeig  (None, 231, 349, 75)     11333     \n",
      " hts)                                                            \n",
      "                                                                 \n",
      " cluster_max_pooling2d (Clus  (None, 77, 116, 75)      0         \n",
      " terWeights)                                                     \n",
      "                                                                 \n",
      " cluster_conv2d_1 (ClusterWe  (None, 77, 116, 135)     506393    \n",
      " ights)                                                          \n",
      "                                                                 \n",
      " cluster_max_pooling2d_1 (Cl  (None, 25, 38, 135)      0         \n",
      " usterWeights)                                                   \n",
      "                                                                 \n",
      " cluster_dropout (ClusterWei  (None, 25, 38, 135)      0         \n",
      " ghts)                                                           \n",
      "                                                                 \n",
      " cluster_conv2d_2 (ClusterWe  (None, 25, 38, 75)       506333    \n",
      " ights)                                                          \n",
      "                                                                 \n",
      " cluster_max_pooling2d_2 (Cl  (None, 8, 12, 75)        0         \n",
      " usterWeights)                                                   \n",
      "                                                                 \n",
      " cluster_dropout_1 (ClusterW  (None, 8, 12, 75)        0         \n",
      " eights)                                                         \n",
      "                                                                 \n",
      " cluster_flatten (ClusterWei  (None, 7200)             0         \n",
      " ghts)                                                           \n",
      "                                                                 \n",
      " cluster_dense (ClusterWeigh  (None, 45)               648053    \n",
      " ts)                                                             \n",
      "                                                                 \n",
      " cluster_dropout_2 (ClusterW  (None, 45)               0         \n",
      " eights)                                                         \n",
      "                                                                 \n",
      " cluster_dense_2 (ClusterWei  (None, 4)                372       \n",
      " ghts)                                                           \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,672,484\n",
      "Trainable params: 836,429\n",
      "Non-trainable params: 836,055\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sparsity_clustered_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model sparsity:\n",
      "\n",
      "kernel:0: 50.03% sparsity  (2814/5625)\n",
      "kernel:0: 50.05% sparsity  (126694/253125)\n",
      "kernel:0: 50.14% sparsity  (126921/253125)\n",
      "kernel:0: 51.40% sparsity  (166547/324000)\n",
      "kernel:0: 50.00% sparsity  (90/180)\n",
      "\n",
      "Model clusters:\n",
      "\n",
      "conv2d/kernel:0: 8 clusters \n",
      "conv2d_1/kernel:0: 8 clusters \n",
      "conv2d_2/kernel:0: 8 clusters \n",
      "dense/kernel:0: 8 clusters \n",
      "dense_2/kernel:0: 8 clusters \n"
     ]
    }
   ],
   "source": [
    "sparsity_clustered_model = tfmot.clustering.keras.strip_clustering(sparsity_clustered_model)\n",
    "\n",
    "print(\"Model sparsity:\\n\")\n",
    "print_model_weights_sparsity(sparsity_clustered_model)\n",
    "\n",
    "print(\"\\nModel clusters:\\n\")\n",
    "print_model_weight_clusters(sparsity_clustered_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparsity_clustered_model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=opt,\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 2s 22ms/step - loss: 0.7012 - accuracy: 0.7550\n",
      "Dokładność modelu przerzedzonego: 0.7549778819084167\n"
     ]
    }
   ],
   "source": [
    "_, pruned_model_accuracy = sparsity_clustered_model.evaluate(data_test, target_test_to_categorical, verbose=1)\n",
    "print('Dokładność modelu przerzedzonego:', pruned_model_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18f5bd19e6ca2f9b4688cbd041a40a785e76d3bff74ae448065bdd2b714a1971"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
